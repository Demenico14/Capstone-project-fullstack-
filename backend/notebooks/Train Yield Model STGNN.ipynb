{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ST-GNN Yield Prediction Training\n",
    "\n",
    "Trains a Spatio-Temporal Graph Neural Network for tobacco yield prediction.\n",
    "\n",
    "**Architecture:**\n",
    "- Graph Convolution: Models spatial relationships between Sensor_1 and Sensor_2\n",
    "- Temporal Convolution: Captures time series patterns over 7-day windows\n",
    "- Fusion & Prediction: Combines spatial-temporal features for yield regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ml_pipeline.models.st_gnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mml_pipeline\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mst_gnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_model\n\u001b[32m     19\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ml_pipeline.models.st_gnn'"
     ]
    }
   ],
   "source": [
    "# 0) Setup\n",
    "import sys\n",
    "sys.path.append('/Users/dominicmushayi/Documents/Capstone /Project/backend')\n",
    "import os, json, warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from ml_pipeline.models.st_gnn import create_model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Paths\n",
    "REPO_ROOT = '/Users/dominicmushayi/Documents/Capstone /Project'\n",
    "DATA_CSV = os.path.join(REPO_ROOT, 'notebooks/cropiot.sensor_data.csv')\n",
    "HARVEST_CSV = os.path.join(REPO_ROOT, 'notebooks/actual_yield.csv')\n",
    "MODELS_DIR = '/Users/dominicmushayi/Documents/GitHub/Capstone-project-fullstack-/backend/ml_pipeline/models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "NDFrame.fillna() got an unexpected keyword argument 'method'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sensor \u001b[38;5;129;01min\u001b[39;00m df[\u001b[33m'\u001b[39m\u001b[33msensor_id\u001b[39m\u001b[33m'\u001b[39m].unique():\n\u001b[32m     13\u001b[39m     mask = df[\u001b[33m'\u001b[39m\u001b[33msensor_id\u001b[39m\u001b[33m'\u001b[39m] == sensor\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     df.loc[mask, \u001b[33m'\u001b[39m\u001b[33mph\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mph\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mffill\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.fillna(\u001b[32m6.0\u001b[39m)\n\u001b[32m     15\u001b[39m     df.loc[mask, \u001b[33m'\u001b[39m\u001b[33msoil_moisture\u001b[39m\u001b[33m'\u001b[39m] = df.loc[mask, \u001b[33m'\u001b[39m\u001b[33msoil_moisture\u001b[39m\u001b[33m'\u001b[39m].fillna(method=\u001b[33m'\u001b[39m\u001b[33mffill\u001b[39m\u001b[33m'\u001b[39m).fillna(\u001b[32m50\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df.sensor_id.unique())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sensors\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: NDFrame.fillna() got an unexpected keyword argument 'method'"
     ]
    }
   ],
   "source": [
    "# Load sensor data\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "df.columns = df.columns.str.strip()\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.replace(-999.0, np.nan, inplace=True)\n",
    "\n",
    "# Clean and sort\n",
    "df = df.dropna(subset=['temperature', 'humidity']).copy()\n",
    "df = df.sort_values(['sensor_id', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Forward fill pH and soil_moisture\n",
    "for sensor in df['sensor_id'].unique():\n",
    "    mask = df['sensor_id'] == sensor\n",
    "    df.loc[mask, 'ph'] = df.loc[mask, 'ph'].fillna(method='ffill').fillna(6.0)\n",
    "    df.loc[mask, 'soil_moisture'] = df.loc[mask, 'soil_moisture'].fillna(method='ffill').fillna(50)\n",
    "\n",
    "print(f'Loaded {len(df)} rows from {len(df.sensor_id.unique())} sensors')\n",
    "print(f'Date range: {df.timestamp.min()} to {df.timestamp.max()}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3996d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Create Sequence Dataset\n",
    "\n",
    "ST-GNN needs sequences: (batch, seq_len, num_nodes, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "feature_names = ['temperature', 'humidity', 'soil_moisture', 'ph']\n",
    "\n",
    "# Add VPD\n",
    "df['vpd'] = 0.611 * np.exp((17.502 * df['temperature']) / (df['temperature'] + 240.97)) * (1 - df['humidity'] / 100)\n",
    "feature_names.append('vpd')\n",
    "\n",
    "# Normalize features per sensor\n",
    "feature_stats = {}\n",
    "for feat in feature_names:\n",
    "    feature_stats[feat] = {'mean': df[feat].mean(), 'std': df[feat].std()}\n",
    "    df[feat] = (df[feat] - feature_stats[feat]['mean']) / (feature_stats[feat]['std'] + 1e-8)\n",
    "\n",
    "print(f'Using {len(feature_names)} features: {feature_names}')\n",
    "print('Feature stats:', json.dumps({k: {sk: round(sv, 2) for sk, sv in v.items()} for k, v in feature_stats.items()}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated yields (calibrated with actual data)\n",
    "if os.path.exists(HARVEST_CSV):\n",
    "    ydf = pd.read_csv(HARVEST_CSV)\n",
    "    ydf.columns = ydf.columns.str.strip()\n",
    "    ydf['yield_kg_per_ha'] = ydf['yield_kg_per_ha'].astype(float)\n",
    "    min_y, max_y = ydf['yield_kg_per_ha'].min(), ydf['yield_kg_per_ha'].max()\n",
    "    print(f'Calibrating simulation with actual yield range: {min_y:.0f} - {max_y:.0f} kg/ha')\n",
    "else:\n",
    "    min_y, max_y = 1500, 3500\n",
    "\n",
    "# Simple yield simulation based on normalized features\n",
    "def simulate_yield(row):\n",
    "    # Denormalize for scoring\n",
    "    temp = row['temperature'] * feature_stats['temperature']['std'] + feature_stats['temperature']['mean']\n",
    "    humid = row['humidity'] * feature_stats['humidity']['std'] + feature_stats['humidity']['mean']\n",
    "    moist = row['soil_moisture'] * feature_stats['soil_moisture']['std'] + feature_stats['soil_moisture']['mean']\n",
    "    ph_val = row['ph'] * feature_stats['ph']['std'] + feature_stats['ph']['mean']\n",
    "    \n",
    "    # Optimal conditions\n",
    "    temp_score = max(0, 1 - abs(temp - 26) / 15)\n",
    "    humid_score = max(0, 1 - abs(humid - 55) / 30)\n",
    "    moist_score = max(0, 1 - abs(moist - 70) / 30)\n",
    "    ph_score = max(0, 1 - abs(ph_val - 6.0) / 1.5)\n",
    "    \n",
    "    score = (temp_score + humid_score + moist_score + ph_score) / 4\n",
    "    base_yield = min_y + score * (max_y - min_y)\n",
    "    noise = np.random.normal(0, 0.08)\n",
    "    return base_yield * (1 + noise)\n",
    "\n",
    "df['yield_kg_per_ha'] = df.apply(simulate_yield, axis=1).clip(min_y, max_y)\n",
    "print(f\"Yield range: {df['yield_kg_per_ha'].min():.0f} - {df['yield_kg_per_ha'].max():.0f} kg/ha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for ST-GNN\n",
    "SEQ_LEN = 168  # 7 days of hourly data\n",
    "sensor_ids = sorted(df['sensor_id'].unique())\n",
    "num_nodes = len(sensor_ids)\n",
    "sensor_to_idx = {s: i for i, s in enumerate(sensor_ids)}\n",
    "\n",
    "print(f'Creating sequences of length {SEQ_LEN} hours ({SEQ_LEN/24:.1f} days)')\n",
    "print(f'Sensors: {sensor_ids} -> node indices {list(sensor_to_idx.values())}')\n",
    "\n",
    "# Group by sensor and create sliding windows\n",
    "sequences = []\n",
    "targets = []\n",
    "\n",
    "for sensor in sensor_ids:\n",
    "    sensor_df = df[df['sensor_id'] == sensor].reset_index(drop=True)\n",
    "    \n",
    "    for i in range(len(sensor_df) - SEQ_LEN):\n",
    "        seq = sensor_df.iloc[i:i+SEQ_LEN]\n",
    "        target_row = sensor_df.iloc[i+SEQ_LEN]\n",
    "        \n",
    "        # Extract features for this sequence\n",
    "        seq_features = seq[feature_names].values  # (SEQ_LEN, num_features)\n",
    "        \n",
    "        # Create node features: replicate this sensor's sequence to all nodes\n",
    "        # (Other nodes will have zeros, filled by adjacency during GCN)\n",
    "        node_features = np.zeros((SEQ_LEN, num_nodes, len(feature_names)))\n",
    "        node_idx = sensor_to_idx[sensor]\n",
    "        node_features[:, node_idx, :] = seq_features\n",
    "        \n",
    "        sequences.append(node_features)\n",
    "        targets.append(target_row['yield_kg_per_ha'])\n",
    "\n",
    "sequences = np.array(sequences)  # (N, SEQ_LEN, num_nodes, num_features)\n",
    "targets = np.array(targets)      # (N,)\n",
    "\n",
    "print(f'Created {len(sequences)} sequences')\n",
    "print(f'Sequence shape: {sequences.shape}')\n",
    "print(f'Target shape: {targets.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Create Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple adjacency: all sensors connected (fully connected graph)\n",
    "# You can customize this based on physical sensor locations\n",
    "adj_matrix = torch.ones(num_nodes, num_nodes)\n",
    "print(f'Adjacency matrix shape: {adj_matrix.shape}')\n",
    "print('Adjacency matrix (1=connected):')\n",
    "print(adj_matrix.numpy().astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Create PyTorch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YieldDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = torch.FloatTensor(sequences)\n",
    "        self.targets = torch.FloatTensor(targets).unsqueeze(1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n",
    "\n",
    "# Split: 80% train, 20% test (time-aware)\n",
    "split_idx = int(len(sequences) * 0.8)\n",
    "train_sequences, test_sequences = sequences[:split_idx], sequences[split_idx:]\n",
    "train_targets, test_targets = targets[:split_idx], targets[split_idx:]\n",
    "\n",
    "train_dataset = YieldDataset(train_sequences, train_targets)\n",
    "test_dataset = YieldDataset(test_sequences, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f'Train: {len(train_dataset)} samples')\n",
    "print(f'Test: {len(test_dataset)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Initialize ST-GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model_type': 'stgnn',\n",
    "    'num_features': len(feature_names),\n",
    "    'num_nodes': num_nodes,\n",
    "    'hidden_dim': 32,\n",
    "    'num_gcn_layers': 2,\n",
    "    'tcn_channels': [32, 32],\n",
    "    'kernel_size': 3,\n",
    "    'dropout': 0.3\n",
    "}\n",
    "\n",
    "model = create_model(config).to(device)\n",
    "adj_matrix = adj_matrix.to(device)\n",
    "\n",
    "print(f'\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "EARLY_STOP_PATIENCE = 20\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print('Starting training...\\n')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch_x, adj_matrix)\n",
    "        loss = criterion(pred, batch_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            pred = model(batch_x, adj_matrix)\n",
    "            loss = criterion(pred, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(test_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), os.path.join(MODELS_DIR, 'stgnn_best.pth'))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    \n",
    "    if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "        print(f'\\nEarly stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "print(f'\\nBest validation loss: {best_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(os.path.join(MODELS_DIR, 'stgnn_best.pth')))\n",
    "model.eval()\n",
    "\n",
    "# Predictions\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        pred = model(batch_x, adj_matrix)\n",
    "        all_preds.append(pred.cpu().numpy())\n",
    "        all_targets.append(batch_y.numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "r2 = r2_score(all_targets, all_preds)\n",
    "rmse = math.sqrt(mean_squared_error(all_targets, all_preds))\n",
    "mae = mean_absolute_error(all_targets, all_preds)\n",
    "\n",
    "print(f'\\n=== Test Set Results ===')\n",
    "print(f'R² Score: {r2:.4f}')\n",
    "print(f'RMSE: {rmse:.2f} kg/ha')\n",
    "print(f'MAE: {mae:.2f} kg/ha')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training History')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(all_targets, all_preds, alpha=0.5)\n",
    "plt.plot([all_targets.min(), all_targets.max()], [all_targets.min(), all_targets.max()], 'r--')\n",
    "plt.xlabel('Actual Yield (kg/ha)')\n",
    "plt.ylabel('Predicted Yield (kg/ha)')\n",
    "plt.title(f'ST-GNN Predictions (R²={r2:.3f})')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Save Model and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "    'feature_names': feature_names,\n",
    "    'feature_stats': feature_stats,\n",
    "    'sensor_to_idx': sensor_to_idx,\n",
    "    'adj_matrix': adj_matrix.cpu().numpy().tolist(),\n",
    "    'seq_len': SEQ_LEN\n",
    "}, os.path.join(MODELS_DIR, 'stgnn_yield_model.pth'))\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_type': 'ST-GNN',\n",
    "    'training_date': datetime.utcnow().isoformat(),\n",
    "    'test_r2_score': float(r2),\n",
    "    'test_rmse': float(rmse),\n",
    "    'test_mae': float(mae),\n",
    "    'num_features': len(feature_names),\n",
    "    'features': feature_names,\n",
    "    'num_nodes': num_nodes,\n",
    "    'seq_len': SEQ_LEN,\n",
    "    'training_samples': len(train_dataset),\n",
    "    'test_samples': len(test_dataset),\n",
    "    'model_params': sum(p.numel() for p in model.parameters())\n",
    "}\n",
    "\n",
    "with open(os.path.join(MODELS_DIR, 'stgnn_metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print('\\n✓ Model and metadata saved!')\n",
    "print(f'  - Model: {MODELS_DIR}/stgnn_yield_model.pth')\n",
    "print(f'  - Metadata: {MODELS_DIR}/stgnn_metadata.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
